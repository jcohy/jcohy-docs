== 设计 Typeahead Suggestion

让我们设计一个实时建议服务，它会在用户输入文本进行搜索时向他们推荐术语。
类似服务：自动推荐、Typeahead 搜索难度：中

[[what_is_typehead_suggestion]]
=== 1. 什么是Typeahead Suggestion?

Typeahead suggestions enable users to search for known and frequently searched terms.
As the user types into the search box, it tries to predict the query based on the characters the user has entered and gives a list of suggestions to complete the query.
Typeahead suggestions help the user to articulate their search queries better.
It’s not about speeding up the search process but rather about guiding the users and lending them a helping hand in constructing their search query.

Typeahead建议使用户能够搜索已知的和经常搜索的术语。
当用户在搜索框中输入内容时，它会尝试根据用户输入的字符预测用户的查询信息，并提供完整的查询建议列表。
提前键入建议可帮助用户更好地完成其搜索查询。
这不是为了加快搜索过程，而是为了指导用户并帮助他们构建搜索查询。


[[requirements_and_goals_of_the_system]]
=== 2.	需求和系统目标

*功能性需求:* 当用户输入查询条件时，我们的服务应该提供10条与用户输入的查询相关的术语。

*非功能性需求:* 推荐的查询术语应该是即时的出现，用户应该能够在200ms内看到系统提供的建议的内容。

[[basic_system_design_and_algorithm]]
=== 3. 基本系统设计和算法

我们将要解决的问题是，要存储大量的字符串，以至于用户可以使用任意前缀进行搜索。我们的服务将会根据给到的前缀进行匹配，建议下一条搜索用的术语。
例如，如果我们的数据包含以下术语：‘cap‘，’cat‘，’captain‘，或 ’captial‘，当用户输入‘cap’时，我们的系统应该建议使用‘cap‘，’captain‘，和 ’captial‘。

由于我们必须以最小的延迟处理大量查询，因此我们需要提出一种可以有效存储数据的方案，以便可以快速查询数据。
因此我们不能依赖某些数据库；我们需要将索引以高效的数据结构存储在内存中。

可以满足我们目的的最合适的数据结构之一是Trie（发音为“try”）。
trie 是一种树状数据结构，用于存储短语，其中每个节点以顺序方式存储短语的字符。
例如，如果我们需要以trie型数据结构存储数据：“cap，cat，caption，captain，capital”，它看起来如下所示：

image::D:/OneDrive/桌面/system design/Typeahead/tire_structure.png[]

现在如果用户已经输入 'cap'，我们的服务会遍历trie型数据结构到节点'p'，找到以'cap'为前缀的所有术语。（例如，'cap-tion'，'cap-ital'等）

我们可以合并只有一个分支的节点来节约存储空间。此时存储数据的trie型数据结构如下：

image::D:/OneDrive/桌面/system design/Typeahead/trie_merge_node.png[]

*我们应该在tire型数据结构中区分大小写吗？* 为了搜索简单，在此假定我们不区分大小写。

*Should we have case insensitive trie?* For simplicity and search use-case, let’s assume our data is case insensitive.

*如何查找顶层建议？* 现在我们可以找到所有给定前缀的术语，我们如何知道应该建议的前 10 个术语是什么？
一个简单的解决方案是在每个尾节点保存搜索次数，例如，如果用户搜索了 “CAPTAIN” 100 次，搜索了 “CAPTION” 500次，我们可以将此数字与短语的最后一个字符保存在一起。
因此，现在如果用户输入了“CAP”，我们知道前缀“CAP”下搜索最多的单词是“CAPTION”。
因此，给定一个前缀，我们可以遍历其下的子树以找到最适合的建议。

*给定一个前缀，遍历其子树需要多少时间？* 考虑到我们需要索引的数据量，我们应该期望一棵巨大的树。
即使遍历子树也需要很长时间，例如，短语“system design interview questions”的深度是30。
由于我们有非常严格的延迟要求，因此我们需要提高查询效率。

*我们可以存储每个节点的热门推荐吗？* 这肯定可以加快我们的搜索速度，但需要大量额外的存储空间。
我们可以在每个节点上存储前 10 个推荐，可以将其返回给用户。我们必须承受存储容量的大幅增加才能达到所需的效率。

*Can we store top suggestions with each node?* This can surely speed up our searches but will require a lot of extra storage.
We can store top 10 suggestions at each node that we can return to the user.
We have to bear the big increase in our storage capacity to achieve the required efficiency.

We can optimize our storage by storing only references of the terminal nodes rather than storing the entire phrase.
To find the suggested terms we need to traverse back using the parent reference from the terminal node.
We will also need to store the frequency with each reference to keep track of top suggestions.

*How would we build this trie?* We can efficiently build our trie bottom up.
Each parent node will recursively call all the child nodes to calculate their top suggestions and their counts.
Parent nodes will combine top suggestions from all of their children to determine their top suggestions.

*How to update the trie?* Assuming five billion searches every day, which would give us approximately 60K queries per second.
If we try to update our trie for every query it’ll be extremely resource intensive and this can hamper our read requests, too.
One solution to handle this could be to update our trie offline after a certain interval.

As the new queries come in we can log them and also track their frequencies.
Either we can log every query or do sampling and log every 1000th query.
For example, if we don’t want to show a term which is searched for less than 1000 times, it’s safe to log every 1000th searched term.

We can have a https://en.wikipedia.org/wiki/MapReduce[Map-Reduce (MR)] set-up to process all the logging data periodically say every hour.
These MR jobs will calculate frequencies of all searched terms in the past hour.
We can then update our trie with this new data.
We can take the current snapshot of the trie and update it with all the new terms and their frequencies.
We should do this offline as we don’t want our read queries to be blocked by update trie requests.
We can have two options:

. We can make a copy of the trie on each server to update it offline.
Once done we can switch to start using it and discard the old one.
. Another option is we can have a master-slave configuration for each trie server.
We can update slave while the master is serving traffic.
Once the update is complete, we can make the slave our new master.
We can later update our old master, which can then start serving traffic, too.

How can we update the frequencies of typeahead suggestions?
Since we are storing frequencies of our typeahead suggestions with each node, we need to update them too.
We can update only differences in frequencies rather than recounting all search terms from scratch.
If we’re keeping count of all the terms searched in last 10 days, we’ll need to subtract the counts from the time period no longer included and add the counts for the new time period being included.
We can add and subtract frequencies based on https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average[ Exponential MovingAverage (EMA)] of each term.
In EMA, we give more weight to the latest data.
It’s also known as the exponentially weighted moving average.

After inserting a new term in the trie, we’ll go to the terminal node of the phrase and increase its frequency.
Since we’re storing the top 10 queries in each node, it is possible that this particular search term jumped into the top 10 queries of a few other nodes.
So, we need to update the top 10 queries of those nodes then.
We have to traverse back from the node to all the way up to the root.
For every parent, we check if the current query is part of the top 10. If so, we update the corresponding frequency.
If not, we check if the current query’s frequency is high enough to be a part of the top 10. If so, we insert this new term and remove the term with the lowest frequency.

*How can we remove a term from the trie?* Let’s say we have to remove a term from the trie because of some legal issue or hate or piracy etc.
We can completely remove such terms from the trie when the regular update happens, meanwhile, we can add a filtering layer on each server which will remove any such term before sending them to users.

*What could be different ranking criteria for suggestions?* In addition to a simple count, for terms ranking, we have to consider other factors too, e.g., freshness, user location, language, demographics, personal history etc.

[[permanent_storage_of_the_trie]]
=== 4.	Permanent Storage of the Trie

*How to store trie in a file so that we can rebuild our trie easily - this will be needed when a machine restarts?*
We can take a snapshot of our trie periodically and store it in a file.
This will enable us to rebuild a trie if the server goes down.
To store, we can start with the root node and save the trie level-by-level.
With each node, we can store what character it contains and how many children it has.
Right after each node, we should put all of its children.
Let’s assume we have the following trie:

If we store this trie in a file with the above-mentioned scheme, we will have: “C2,A2,R1,T,P,O1,D”.
From this, we can easily rebuild our trie.

If you’ve noticed, we are not storing top suggestions and their counts with each node.
It is hard to store this information; as our trie is being stored top down, we don’t have child nodes created before the parent, so there is no easy way to store their references.
For this, we have to recalculate all the top terms with counts.
This can be done while we are building the trie.
Each node will calculate its top suggestions and pass it to its parent.
Each parent node will merge results from all of its children to figure out its top suggestions.

[[sacle_estimation]]
=== 5. Scale Estimation

If we are building a service that has the same scale as that of Google we can expect 5 billion searches every day, which would give us approximately 60K queries per second.

Since there will be a lot of duplicates in 5 billion queries, we can assume that only 20% of these will be unique.
If we only want to index the top 50% of the search terms, we can get rid of a lot of less frequently searched queries.
Let’s assume we will have 100 million unique terms for which we want to build an index.

*Storage Estimation:* If on the average each query consists of 3 words and if the average length of a word is 5 characters, this will give us 15 characters of average query size.
Assuming we need 2 bytes to store a character, we will need 30 bytes to store an average query.
So total storage we will need:

[source,text]
----
100 million * 30 bytes => 3 GB
----

We can expect some growth in this data every day, but we should also be removing some terms that are not searched anymore.
If we assume we have 2% new queries every day and if we are maintaining our index for the last one year, total storage we should expect:

[source,text]
----
3GB + (0.02 * 3 GB * 365 days) => 25 GB
----

[[data_partition]]
=== 6. Data Partition

Although our index can easily fit on one server, we can still partition it in order to meet our requirements of higher efficiency and lower latencies.
How can we efficiently partition our data to distribute it onto multiple servers?

a. *Range Based Partitioning:* What if we store our phrases in separate partitions based on their first letter.
So we save all the terms starting with the letter ‘A’ in one partition and those that start with the letter ‘B’ into another partition and so on.
We can even combine certain less frequently occurring letters into one database partition.
We should come up with this partitioning scheme statically so that we can always store and search terms in a predictable manner.
+
The main problem with this approach is that it can lead to unbalanced servers, for instance, if we decide to put all terms starting with the letter ‘E’ into a DB partition, but later we realize that we have too many terms that start with letter ‘E’ that we can’t fit into one DB partition.
+
We can see that the above problem will happen with every statically defined scheme.
It is not possible to calculate if each of our partitions will fit on one server statically.

b. *Partition based on the maximum capacity of the server:* Let’s say we partition our trie based on the maximum memory capacity of the servers.
We can keep storing data on a server as long as it has memory available.
Whenever a sub-tree cannot fit into a server, we break our partition there to assign that range to this server and move on the next server to repeat this process.
Let’s say if our first trie server can store all terms from ‘A’ to ‘AABC’, which mean our next server will store from ‘AABD’ onwards.
If our second server could store up to ‘BXA’, the next server will start from ‘BXB’, and so on.
We can keep a hash table to quickly access this partitioning scheme: +
Server 1, A-AABC +
Server 2, AABD-BXA +
Server 3, BXB-CDA +
For querying, if the user has typed ‘A’ we have to query both server 1 and 2 to find the top suggestions.
When the user has typed ‘AA’, we still have to query server 1 and 2, but when the user has typed ‘AAA’ we only need to query server 1.
+
We can have a load balancer in front of our trie servers which can store this mapping and redirect traffic.
Also, if we are querying from multiple servers, either we need to merge the results at the server side to calculate overall top results or make our clients do that.
If we prefer to do this on the server side, we need to introduce another layer of servers between load balancers and trie severs (let’s call them aggregator).
These servers will aggregate results from multiple trie servers and return the top results to the client.
+
Partitioning based on the maximum capacity can still lead us to hotspots, e.g., if there are a lot of queries for terms starting with ‘cap’, the server holding it will have a high load compared to others.

c. *Partition based on the hash of the term:* Each term will be passed to a hash function, which will generate a server number and we will store the term on that server.
This will make our term distribution random and hence minimize hotspots.
To find typeahead suggestions for a term we have to ask all the servers and then aggregate the results.

[[cache]]
=== 7. Cache

We should realize that caching the top searched terms will be extremely helpful in our service.
There will be a small percentage of queries that will be responsible for most of the traffic.
We can have separate cache servers in front of the trie servers holding most frequently searched terms and their typeahead suggestions.
Application servers should check these cache servers before hitting the trie servers to see if they have the desired searched terms.

We can also build a simple Machine Learning (ML) model that can try to predict the engagement on each suggestion based on simple counting, personalization, or trending data etc., and cache these terms.

[[replication_and_load_balancer]]
=== 8. Replication and Load Balancer

We should have replicas for our trie servers both for load balancing and also for fault tolerance.
We also need a load balancer that keeps track of our data partitioning scheme and redirects traffic based on the prefixes.

[[fault_tolerance]]
=== 9. Fault Tolerance

What will happen when a trie server goes down?
As discussed above we can have a master-slave configuration; if the master dies, the slave can take over after failover.
Any server that comes back up, can rebuild the trie based on the last snapshot.

[[typeahead_client]]
=== 10. Typeahead Client

We can perform the following optimizations on the client to improve user’s experience:

1. The client should only try hitting the server if the user has not pressed any key for 50ms.

2. If the user is constantly typing, the client can cancel the in-progress requests.
3. Initially, the client can wait until the user enters a couple of characters.
4. Clients can pre-fetch some data from the server to save future requests.
5. Clients can store the recent history of suggestions locally.
Recent history has a very high rate of being reused.
6. Establishing an early connection with the server turns out to be one of the most important factors.
As soon as the user opens the search engine website, the client can open a connection with the server.
So when a user types in the first character, the client doesn’t waste time in establishing the connection.
7. The server can push some part of their cache to CDNs and Internet Service Providers (ISPs) for efficiency.

[[personalization]]
=== 11. Personalization

Users will receive some typeahead suggestions based on their historical searches, location, language, etc.
We can store the personal history of each user separately on the server and cache them on the client too.
The server can add these personalized terms in the final set before sending it to the user.
Personalized searches should always come before others.
