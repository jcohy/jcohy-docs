== 设计 Typeahead Suggestion

让我们设计一个实时建议服务，它会在用户输入文本进行搜索时向他们推荐术语。
类似服务：自动推荐、Typeahead 搜索难度：中

[[what_is_typehead_suggestion]]
=== 1. 什么是Typeahead Suggestion?

Typeahead suggestions enable users to search for known and frequently searched terms.
As the user types into the search box, it tries to predict the query based on the characters the user has entered and gives a list of suggestions to complete the query.
Typeahead suggestions help the user to articulate their search queries better.
It’s not about speeding up the search process but rather about guiding the users and lending them a helping hand in constructing their search query.

Typeahead建议使用户能够搜索已知的和经常搜索的术语。
当用户在搜索框中输入内容时，它会尝试根据用户输入的字符预测用户的查询信息，并提供完整的查询建议列表。
提前键入建议可帮助用户更好地完成其搜索查询。
这不是为了加快搜索过程，而是为了指导用户并帮助他们构建搜索查询。


[[requirements_and_goals_of_the_system]]
=== 2.	需求和系统目标

*功能性需求:* 当用户输入查询条件时，我们的服务应该提供10条与用户输入的查询相关的术语。

*非功能性需求:* 推荐的查询术语应该是即时的出现，用户应该能够在200ms内看到系统提供的建议的内容。

[[basic_system_design_and_algorithm]]
=== 3. 基本系统设计和算法

我们将要解决的问题是，要存储大量的字符串，以至于用户可以使用任意前缀进行搜索。我们的服务将会根据给到的前缀进行匹配，建议下一条搜索用的术语。
例如，如果我们的数据包含以下术语：‘cap‘，’cat‘，’captain‘，或 ’captial‘，当用户输入‘cap’时，我们的系统应该建议使用‘cap‘，’captain‘，和 ’captial‘。

由于我们必须以最小的延迟处理大量查询，因此我们需要提出一种可以有效存储数据的方案，以便可以快速查询数据。
因此我们不能依赖某些数据库；我们需要将索引以高效的数据结构存储在内存中。

可以满足我们目的的最合适的数据结构之一是Trie（发音为“try”）。
trie 是一种树状数据结构，用于存储短语，其中每个节点以顺序方式存储短语的字符。
例如，如果我们需要以trie型数据结构存储数据：“cap，cat，caption，captain，capital”，它看起来如下所示：

image::D:/OneDrive/桌面/system design/Typeahead/tire_structure.png[]

现在如果用户已经输入 'cap'，我们的服务会遍历trie型数据结构到节点'p'，找到以'cap'为前缀的所有术语。（例如，'cap-tion'，'cap-ital'等）

我们可以合并只有一个分支的节点来节约存储空间。此时存储数据的trie型数据结构如下：

image::D:/OneDrive/桌面/system design/Typeahead/trie_merge_node.png[]

*我们应该在tire型数据结构中区分大小写吗？* 为了搜索简单，在此假定我们不区分大小写。

*如何查找顶层建议？* 现在我们可以找到所有给定前缀的术语，我们如何知道应该建议的前 10 个术语是什么？
一个简单的解决方案是在每个尾节点保存搜索次数，例如，如果用户搜索了 “CAPTAIN” 100 次，搜索了 “CAPTION” 500次，我们可以将此数字与短语的最后一个字符保存在一起。
因此，现在如果用户输入了“CAP”，我们知道前缀“CAP”下搜索最多的单词是“CAPTION”。
因此，给定一个前缀，我们可以遍历其下的子树以找到最适合的建议。

*给定一个前缀，遍历其子树需要多少时间？* 考虑到我们需要索引的数据量，我们应该期望一棵巨大的树。
即使遍历子树也需要很长时间，例如，短语“system design interview questions”的深度是30。
由于我们有非常严格的延迟要求，因此我们需要提高查询效率。

*我们可以存储每个节点的热门推荐吗？* 这肯定可以加快我们的搜索速度，但需要大量额外的存储空间。
我们可以在每个节点上存储前 10 个推荐，可以将其返回给用户。我们必须承受存储容量的大幅增加才能达到所需的效率。

我们可以通过存储终端节点的引用而不是存储整个短语来优化存储。要找到推荐项，我们需要使用终端节点中的父引用进行后置遍历。
我们还需要存储每个引用的频率，以跟踪热门推荐。

*将如何构建这个trie型数据结构？* 我们可以有效地自下而上地构建我们的trie。每个父节点将以递归方式调用所有子节点，以计算其热门推荐和计数。
父节点将组合来自其所有子节点的热门推荐，以确定最终的热门推荐。

*如何更新trie型数据结构？* 假设每天有50亿次搜索，这将会有每秒大约60K的查询。
如果我们尝试为每个查询更新我们的trie，它将是非常消耗资源，这也可能会阻塞我们的读取请求。
处理此问题的一种解决方案是在一定间隔后离线更新我们的 trie 型数据结构。

当新查询进来时，我们可以记录它们并跟踪它们的查询频率。
我们可以记录每个查询，也可以进行采样并记录每第 1000 个查询。
例如，如果我们不想显示搜索次数少于 1000 次的术语，则每记录第 1000 个搜索的术语是安全的。

我们可以设置一个 https://en.wikipedia.org/wiki/MapReduce[Map-Reduce (MR)]来定期处理所有日志中记录的数据，比如每小时一次。
这些 MR 作业将计算过去一小时内所有搜索词的频率。然后，我们可以用这些新数据更新 tire 数据结构。
我们可以拍摄 trie 的当前快照，并使用所有新术语及其频率对其进行更新。
我们应该离线执行此操作，因为我们不希望更新 trie 树结构的操作阻塞读取查询。
我们可以有两种选择：

. 我们可以在每台服务器上复制一份 trie 数据以离线更新它。更新完成后，我们可以使用新的数据并丢弃旧数据。
. 另一种选择是我们可以为每个trie服务器配置主从服务。我们可以使用master为流量提供服务的同时，更新slave上的数据。
更新完成后，我们可以使slave服务器成为我们的新的master服务器。我们稍后可以更新旧master，然后它又开始为流量提供服务。


如何更新typeahead推荐的频率？因为我们保存了每个节点推荐的频率，因此我们也需要更新它们。
我们只需更新频率的差异，而不是从头开始重新计算所有搜索词的搜索次数。
如果我们保留过去 10 天内搜索的所有字词的计数，则需要从不再包含的时间段中减去计数，并将包含的新时间段的计数相加。
我们可以根据每个项的 https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average[指数移动平均值（EMA）]来加减频率。
在EMA中，我们更加重视最新数据。它也被称为指数加权移动平均值。



在trie数据结构中插入一个新术语后，我们将转到短语的终端节点并增加其频率。
由于我们在每个节点中存储前 10 个查询，因此此特定搜索词可能会跳入其他几个节点的前 10 个查询。
因此，我们需要更新这些节点的前 10 个查询。
我们必须从节点遍历到根节点。对于每个父查询，我们检查当前查询是否属于前 10 名的一部分。如果是这样，我们会更新相应的频率。
如果不是，我们会检查当前查询的频率是否足够大，可以成为前 10 名的一部分。如果是这样，我们将插入此新术语并删除频率最低的术语。


*我们如何从trie中删除一个术语？* 假设由于一些法律或仇恨或盗版等问题，我们必须从trie结构中删除一个术语。当定期更新发生时，
我们可以从 trie 结构中完全删除这些术语，同时，我们可以在每个服务器中添加一个过滤层，该过滤层将在此类术语发送给用户之前将其删除。

*推荐的排名标准可能有所不同？*  除了简单的计数之外，对于术语排名，我们还必须考虑其他因素，例如新鲜度、用户位置、语言、人口统计、个人历史等。


[[permanent_storage_of_the_trie]]
=== 4.	Trie的持久化


*如何将 trie 结构存储在文件中，当机器重新启动以便我们可以轻松重建我们的 trie 结构？*
我们可以定期拍摄trie结构的快照并将其存储在文件中。这将使我们能够在服务器出现故障时重建 trie。
为了存储数据，我们可以从根节点开始，逐级保存trie。对于每个节点，我们可以存储它包含的字符以及它有多少个子节点。
在每个节点之后，我们应该放置它的所有子节点。
让假设trie结构如下所示：

image::xxx[]

如果我们用上面的方案存储trie结构，我们将获得如下形式的内容：“C2,A2,R1,T,P,O1,D”，使用这中形式的数据，可以很方便的重建trie结构。

如果您已经注意到，我们不会存储每个节点的热门推荐及其计数。
因为很难存储这些信息;由于我们的 trie 是自上而下存储的，因此我们不会在父节点之前创建子节点，
因此，没有简单的方法来存储它们的引用。为此，我们必须重新计算所有带有计数的顶级项。
这可以在我们构建 trie 结构时完成。每个节点将计算其热门推荐并将其传递给其父节点。
每个父节点将合并其所有子节点的结果，以找出其主要推荐。


If you’ve noticed, we are not storing top suggestions and their counts with each node.
It is hard to store this information; as our trie is being stored top down, we don’t have child nodes created before the parent,
so there is no easy way to store their references.For this, we have to recalculate all the top terms with counts.
This can be done while we are building the trie.
Each node will calculate its top suggestions and pass it to its parent.
Each parent node will merge results from all of its children to figure out its top suggestions.

[[sacle_estimation]]
=== 5. Scale Estimation

If we are building a service that has the same scale as that of Google we can expect 5 billion searches every day, which would give us approximately 60K queries per second.

Since there will be a lot of duplicates in 5 billion queries, we can assume that only 20% of these will be unique.
If we only want to index the top 50% of the search terms, we can get rid of a lot of less frequently searched queries.
Let’s assume we will have 100 million unique terms for which we want to build an index.

*Storage Estimation:* If on the average each query consists of 3 words and if the average length of a word is 5 characters, this will give us 15 characters of average query size.
Assuming we need 2 bytes to store a character, we will need 30 bytes to store an average query.
So total storage we will need:

[source,text]
----
100 million * 30 bytes => 3 GB
----

We can expect some growth in this data every day, but we should also be removing some terms that are not searched anymore.
If we assume we have 2% new queries every day and if we are maintaining our index for the last one year, total storage we should expect:

[source,text]
----
3GB + (0.02 * 3 GB * 365 days) => 25 GB
----

[[data_partition]]
=== 6. Data Partition

Although our index can easily fit on one server, we can still partition it in order to meet our requirements of higher efficiency and lower latencies.
How can we efficiently partition our data to distribute it onto multiple servers?

a. *Range Based Partitioning:* What if we store our phrases in separate partitions based on their first letter.
So we save all the terms starting with the letter ‘A’ in one partition and those that start with the letter ‘B’ into another partition and so on.
We can even combine certain less frequently occurring letters into one database partition.
We should come up with this partitioning scheme statically so that we can always store and search terms in a predictable manner.
+
The main problem with this approach is that it can lead to unbalanced servers, for instance, if we decide to put all terms starting with the letter ‘E’ into a DB partition, but later we realize that we have too many terms that start with letter ‘E’ that we can’t fit into one DB partition.
+
We can see that the above problem will happen with every statically defined scheme.
It is not possible to calculate if each of our partitions will fit on one server statically.

b. *Partition based on the maximum capacity of the server:* Let’s say we partition our trie based on the maximum memory capacity of the servers.
We can keep storing data on a server as long as it has memory available.
Whenever a sub-tree cannot fit into a server, we break our partition there to assign that range to this server and move on the next server to repeat this process.
Let’s say if our first trie server can store all terms from ‘A’ to ‘AABC’, which mean our next server will store from ‘AABD’ onwards.
If our second server could store up to ‘BXA’, the next server will start from ‘BXB’, and so on.
We can keep a hash table to quickly access this partitioning scheme: +
Server 1, A-AABC +
Server 2, AABD-BXA +
Server 3, BXB-CDA +
For querying, if the user has typed ‘A’ we have to query both server 1 and 2 to find the top suggestions.
When the user has typed ‘AA’, we still have to query server 1 and 2, but when the user has typed ‘AAA’ we only need to query server 1.
+
We can have a load balancer in front of our trie servers which can store this mapping and redirect traffic.
Also, if we are querying from multiple servers, either we need to merge the results at the server side to calculate overall top results or make our clients do that.
If we prefer to do this on the server side, we need to introduce another layer of servers between load balancers and trie severs (let’s call them aggregator).
These servers will aggregate results from multiple trie servers and return the top results to the client.
+
Partitioning based on the maximum capacity can still lead us to hotspots, e.g., if there are a lot of queries for terms starting with ‘cap’, the server holding it will have a high load compared to others.

c. *Partition based on the hash of the term:* Each term will be passed to a hash function, which will generate a server number and we will store the term on that server.
This will make our term distribution random and hence minimize hotspots.
To find typeahead suggestions for a term we have to ask all the servers and then aggregate the results.

[[cache]]
=== 7. Cache

We should realize that caching the top searched terms will be extremely helpful in our service.
There will be a small percentage of queries that will be responsible for most of the traffic.
We can have separate cache servers in front of the trie servers holding most frequently searched terms and their typeahead suggestions.
Application servers should check these cache servers before hitting the trie servers to see if they have the desired searched terms.

We can also build a simple Machine Learning (ML) model that can try to predict the engagement on each suggestion based on simple counting, personalization, or trending data etc., and cache these terms.

[[replication_and_load_balancer]]
=== 8. Replication and Load Balancer

We should have replicas for our trie servers both for load balancing and also for fault tolerance.
We also need a load balancer that keeps track of our data partitioning scheme and redirects traffic based on the prefixes.

[[fault_tolerance]]
=== 9. Fault Tolerance

What will happen when a trie server goes down?
As discussed above we can have a master-slave configuration; if the master dies, the slave can take over after failover.
Any server that comes back up, can rebuild the trie based on the last snapshot.

[[typeahead_client]]
=== 10. Typeahead Client

We can perform the following optimizations on the client to improve user’s experience:

1. The client should only try hitting the server if the user has not pressed any key for 50ms.

2. If the user is constantly typing, the client can cancel the in-progress requests.
3. Initially, the client can wait until the user enters a couple of characters.
4. Clients can pre-fetch some data from the server to save future requests.
5. Clients can store the recent history of suggestions locally.
Recent history has a very high rate of being reused.
6. Establishing an early connection with the server turns out to be one of the most important factors.
As soon as the user opens the search engine website, the client can open a connection with the server.
So when a user types in the first character, the client doesn’t waste time in establishing the connection.
7. The server can push some part of their cache to CDNs and Internet Service Providers (ISPs) for efficiency.

[[personalization]]
=== 11. Personalization

Users will receive some typeahead suggestions based on their historical searches, location, language, etc.
We can store the personal history of each user separately on the server and cache them on the client too.
The server can add these personalized terms in the final set before sending it to the user.
Personalized searches should always come before others.
